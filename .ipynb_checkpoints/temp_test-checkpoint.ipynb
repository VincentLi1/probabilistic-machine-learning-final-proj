{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    # MulticlassNegativeLogLikelihood,\n",
    "    # MulticlassBrierScoreLoss,\n",
    "    MulticlassCalibrationError,\n",
    ")\n",
    "from pathlib import Path\n",
    "from loaders import (\n",
    "    tiny_imagenet_train_loader, \n",
    "    tiny_imagenet_val_loader,\n",
    "    tiny_imagenet_corrupted_loader\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Data --------------------------------------------------------------------\n",
    "BATCH_TRAIN = 128\n",
    "BATCH_VAL  = 128\n",
    "\n",
    "train_loader = tiny_imagenet_train_loader(batch_size=BATCH_TRAIN)\n",
    "val_loader   = tiny_imagenet_val_loader(batch_size=BATCH_VAL)\n",
    "\n",
    "NUM_CLASSES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- α‑BatchNorm --------------------------------------------------------------\n",
    "class AlphaBN(nn.BatchNorm2d):\n",
    "    \"\"\"BatchNorm2d that fuses source & target stats at test‑time.\"\"\"\n",
    "    def __init__(self, num_features, alpha=0.9, **kwargs):\n",
    "        super().__init__(num_features, affine=True, track_running_stats=True, **kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.to(device)\n",
    "        \n",
    "        if self.training:               # standard BN in training\n",
    "            return super().forward(x)\n",
    "\n",
    "        # Evaluation → blend source stats with batch stats\n",
    "        batch_mean = x.mean([0, 2, 3])\n",
    "        batch_var  = x.var([0, 2, 3], unbiased=False)\n",
    "\n",
    "        # print(self.running_mean.device)\n",
    "        # print(self.running_var.device)\n",
    "        # print(batch_mean.device)\n",
    "        # print(batch_var.device)\n",
    "        # print(self.weight.device)\n",
    "        # print(self.bias.device)\n",
    "\n",
    "        # batch_mean is the target mean\n",
    "        # self.running_mean is the source mean\n",
    "        mean = (self.alpha * self.running_mean + (1 - self.alpha) * batch_mean).detach()\n",
    "        var  = (self.alpha * self.running_var  + (1 - self.alpha) * batch_var).detach()\n",
    "\n",
    "        return F.batch_norm(x, mean, var, self.weight, self.bias,\n",
    "                            False, 0.0, self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_alpha_bn(module, alpha=0.9):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.BatchNorm2d):\n",
    "            setattr(module, name, AlphaBN(child.num_features, alpha=alpha))\n",
    "        else:\n",
    "            convert_to_alpha_bn(child, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Core loss ----------------------------------------------------------------\n",
    "def core_loss(logits):\n",
    "    probs = logits.softmax(dim=1)        # (B, C)\n",
    "    m = probs.mean(dim=0)                # (C,)\n",
    "    outer = torch.outer(m, m)\n",
    "    return outer.sum() - outer.diag().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Train baseline -----------------------------------------------------------\n",
    "def train_baseline(epochs=90, lr=0.1):\n",
    "    model = torchvision.models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = None\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\", leave=False):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "        sched.step()\n",
    "\n",
    "        # Periodically validate and save in case of crash\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            val_acc, val_loss, val_ece = evaluate(model, val_loader, print_results=False)\n",
    "            model.train()\n",
    "\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                torch.save(model.state_dict(), f\"./models/bn_train3.pt\")\n",
    "                best_val = val_acc\n",
    "            elif val_loss > best_val_loss + 1:\n",
    "                print(\"Early stop because of degredation in validation loss\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Training loss: {loss.item()}\")\n",
    "            print(f\"Validation loss: {val_loss}\")\n",
    "            print(f\"Validation accuracy: {val_acc}\")\n",
    "            \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Adapt BN affine params on Tiny‑ImageNet‑C --------------------------------\n",
    "def adapt_alpha_bn(model, test_loader, device, alpha=0.9, lr=1e-3, epochs=1):\n",
    "    convert_to_alpha_bn(model, alpha)\n",
    "    model.to(device)\n",
    "    for p in model.parameters(): p.requires_grad_(False)\n",
    "    bn_params = [m.weight for m in model.modules() if isinstance(m, AlphaBN)] + \\\n",
    "                [m.bias   for m in model.modules() if isinstance(m, AlphaBN)]\n",
    "    for p in bn_params: p.requires_grad_(True)\n",
    "    opt = torch.optim.Adam(bn_params, lr=lr)\n",
    "\n",
    "    model.eval()\n",
    "    for _ in range(epochs):\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            loss = core_loss(logits)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Metrics ------------------------------------------------------------------\n",
    "def evaluate(model, loader, print_results=True):\n",
    "    model.eval()\n",
    "    ece = MulticlassCalibrationError(NUM_CLASSES, n_bins=15, norm='l1').to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            probs  = logits.softmax(dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            total_correct += (preds==y).sum()\n",
    "            total_loss += loss_fn(probs, y).item()\n",
    "            total_samples += y.size(0)\n",
    "            ece.update(probs, y)\n",
    "\n",
    "    loss = total_loss / total_samples\n",
    "    accuracy = total_correct/total_samples\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"Accuracy                : {accuracy:.4f}\")\n",
    "        print(f\"Cross-Entropy Loss      : {loss:.4f}\")\n",
    "        print(f\"Expected calibration err: {ece.compute():.4f}\")\n",
    "\n",
    "    print(f\"{total_correct} / {total_samples}\")\n",
    "\n",
    "    return accuracy, loss, ece.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "\n",
      "Training Results\n",
      "\n",
      "Test Results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae0454389cd4decb9f00cbc01dd1633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy                : 0.0038\n",
      "Cross-Entropy Loss      : 0.0419\n",
      "Expected calibration err: 0.5014\n",
      "38 / 10000\n"
     ]
    }
   ],
   "source": [
    "# ---- Full run -----------------------------------------------------------------\n",
    "# 1. Train or load baseline\n",
    "baseline_ckpt = Path('models/bn_baseline_model3.pt')\n",
    "if baseline_ckpt.exists():\n",
    "    print('Loading pretrained model')\n",
    "    model = torchvision.models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(baseline_ckpt, map_location=device))\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('Training model froms scratch')\n",
    "    model = train_baseline(epochs=100)\n",
    "    torch.save(model.state_dict(), baseline_ckpt)\n",
    "\n",
    "print('\\nTraining Results')\n",
    "# accl, loss, ece = evaluate(model, train_loader)\n",
    "\n",
    "print('\\nTest Results')\n",
    "acc, loss, ece = evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01487bb523d84a0792c5c849c6a3be01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy                : 0.2206\n",
      "Cross-Entropy Loss      : 0.0406\n",
      "Expected calibration err: 0.2073\n",
      "2206 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.2206, device='cuda:0'),\n",
       " 0.04058969650268555,\n",
       " tensor(0.2073, device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.5 Test model without adaptation\n",
    "\n",
    "corrupt_loader = tiny_imagenet_corrupted_loader(\n",
    "    corruption='brightness', \n",
    "    severity=1, \n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "acc, loss, ece = evaluate(model, corrupt_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance on Tiny‑ImageNet‑C (after adaptation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c2a16c8ea043b69e0bfe840c9c2225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy                : 0.0050\n",
      "Cross-Entropy Loss      : 0.0419\n",
      "Expected calibration err: 0.9527\n",
      "50 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0050, device='cuda:0'),\n",
       " 0.04188191142082214,\n",
       " tensor(0.9527, device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. α‑BN + Core adaptation (look at brightness-1 for example)\n",
    "adapted_model = adapt_alpha_bn(model, test_loader=corrupt_loader, device=device, alpha=0.9, lr=1e-3, epochs=1)\n",
    "\n",
    "print('\\nPerformance on Tiny‑ImageNet‑C (after adaptation)')\n",
    "acc, loss, ece = evaluate(adapted_model, corrupt_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pml_env_dist] *",
   "language": "python",
   "name": "conda-env-.conda-pml_env_dist-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
